\documentclass{article}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{pdflscape}
\usepackage{longtable}
\usepackage[toc,page]{appendix}
\usepackage{array,booktabs}

\begin{document}

\title{Are assessed stocks a representative sample of fished species in the US?}

\author{Philipp Neubauer, Dragonfly Data Science, Wellington, NZ \\
\and James T. Thorsen, NOAA Northwest Fisheries Science Center, Seattle 
\and            Michael C. Melnychuk, School of Aquatic and Fisheries Science, \\University of Washington, Seattle}

\maketitle

{\bf Target journals}

\begin{enumerate}
\item Fish \& Fisheries
\item ICES JMS
\item Journal of Applied Ecology (should be easy to write for this, but maybe have less of a fisheries audience)
\item Ecological Applications (ditto JAPPL)
\end{enumerate}


\begin{abstract}

At a minimum, fisheries management requires estimating fishery harvest (or fishing rates) such that the fishery will perform well on average.  However, many management agencies also define objectives regarding population status (e.g., biomass relative to biological targets), and these different mandates (estimating harvest rates vs. population status) are often achieved using different types of models.  For example, the United States has a legislative mandate to "end overfishing" by setting harvest limits below biological targets, and has estimated these targets for many species using models that do not estimate population status. By contrast, estimating population status generally requires a statistical model of population dynamics that estimates population scale, which we call a "stock assessment".  Stock assessments are often more costly and resource-intensive than models to estimate harvest limits, and to date only a subset of landed species have stock assessments. Here we quantitatively explore the factors that influence the probability that a previously unassessed stock will receive a stock assessment in the United States. Using a statistical model based on time-to-event analysis and nearly 600 stocks, we quantify the imapct of region, habitat, and life-history the annual probability of being assessed for the first time. Although the majority of landings come from assessed stocks in all regions, less than half of the regionally landed species have a stock assessment. Landings are thus the dominant factor that determines the rate of new assessments, followed by price per kg of landed fish. Nevertheless, we find that the over-all rate at which new stocks are assessed has been increasing since the 1950s, and a number of vulnerable groups such as Rockfish and some Elasmobranchs have a relatively high annual probability of being assessed when controlling for their relativly small tonnage and low price. Given the characteristics of species that are currently un-assessed, our model suggests that the number of assessed stocks will increase slowly in the future, as the landed tonnage and price for the remaining unassessed stocks makes it unlikely that present resources are sufficient to generate stock assessments for these species.

\end{abstract}

<<echo=FALSE>>=
require(knitr)
opts_chunk$set(warning=F, message = FALSE,echo=F,error=FALSE,cache=T, autodep=TRUE)
@

<<preamble,results='hide'>>=
DB <- '~/Dropbox'

# Settings for Jim's machine
if( Sys.info()["effective_user"] == "James.Thorson" ){
  setwd("C:/Users/James.Thorson/Desktop/Project_git/FirstAssessment")
  DB <- 'C:/Users/James.Thorson/Dropbox'
}

source('helper_functs.R')
require("dplyr")
require(xtable)
require("ggplot2")
library(survival)

cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7","#999999")


load(file.path(DB,"First year of assessment/Weibull_model_output.rda"),v=T)

# just for matching
year.table$Region <- year.table$mainregion
year.table$Habitat <- year.table$habitat_MM
year.table$region <- year.table$mainregion
year.table$habitat <- year.table$habitat_MM

coeffs <- tbl_df(get_coef_chains(model.out = a.out, 
                                 coef.names = 'betas',
                                 var.names = c('Length',
                                               'Maximum landings',
                                               'Mean price per kg')))


tau <- tbl_df(get_coef_chains(model.out = a.out, 
                                 coef.names = 'tau',
                                 var.names = 'Rate'))
coeffs$Rate = tau$MCMC
# regressin coeffs are beta
coef_P <- coeffs %>%
  group_by(Parameter) %>%
  summarise(post.mean = mean(exp(MCMC)),
            acc = mean(exp(-MCMC/Rate)),
            post.P = mean(exp(MCMC) > 1))
@

\FloatBarrier
\newpage
\section{Introduction}

It is often said "what gets measured gets managed".  For over XXXX years (historical citation), fisheries scientists have measured human impacts on ocean populations (finfishes and marine invertebrates) with the goal of balancing both the long-term sustainability of human impacts with maximizing harvest (or value) from fishing.  In particular, stock assessments generally involve estimating two measures of human impact: (1) fishing rate, i.e., the instantaneous mortality or annual fraction of the population that is harvested relative to an estimated target level, and (2) the population abundance, i.e., spawning biomass or reproductive output relative to an estimated target level.  The value of these two metrics can be plotted on a 2-dimensional "stock status" plot, and fisheries agencies are increasingly committed to maintaining fished populations in the "healthy" quadrant (i.e., fishing rate below a target level, and population abundance above a target level; citation).

Recently, the National Marine Fisheries Service (NMFS, the agency in charge of science supporting fisheries management in the United States) has committed to "end overfishing" for all marine species within regional fisheries management plans (with exceptions granted in a few circumstances; citation). In the US, overfishing is defined as any stock having annual harvest above limit levels. A target (or limit) harvest can in theory be calculated by combining a target harvest rate with a target population abundance.  However, the vast majority of overfishing limits are currently estimated using methods that do not individually calculate either harvest rate or population abundance (Berkson and Thorson 2015, Newman Berkson Sautoni 2015).  For example, depletion-corrected average catch (DCAC; MacCall 2009) is used to calculate an annual fishing limit for many stocks, but is not used to calculate population abundance. DCAC (and similar methods) can therefore be used to help "end overfishing" but is not otherwise informative about characteristics of the fished population. 

However, conservationists and ecologists will often be more interested in estimating population abundance (or abundance relative to equilibrium conditions) than estimating an overfishing limit (citations).  Estimating abundance generally requires applying a population model to available harvest data and an index of population depletion (either an index proportional to population abundance, or average size/age data);  in these cases, the population model essentially treats the historical fishing as a "depletion experiment" to estimate the fraction of the population that is fished every year.  In the following, we call these population models "stock assessments", although we acknowledge that other authors have used the term "stock assessment" more broadly to also include methods for estimating overfishing limits (e.g., DCAC). Although NMFS has estimated overfishing limits for the vast majority of fishes in US fisheries management plans, a much smaller percentage of fished species have a stock assessment (in our sense). This relative dearth of stock assessment models presumably arises because developing a stock assessment is time-consuming and requires extensive financial resources from NMFS and other interested parties (Geremont ref).

We  argue that stock assessments (population models for estimating absolute abundance for fished species) are important for many applied and theoretical questions regarding marine ecosystems.  In particular, managing population stability (a characteristic of population size), rather than simply annual fishery removals, depends upon measuring population abundance via stock assessment models.  However, there is little previous research regarding which fished species are more or less likely to receive sufficient attention to develop a stock assessment.  Understanding which species are more or less likely to be assessed could be useful for the following three reasons (among others):

1.  Unassessed stocks may receive less attention from the public or fisheries managers regarding management actions. Therefore, if a given taxon is less likely to be assessed, it might also be less likely to receive rapid or public attention when management changes are warrented.

2.  Output from stock assessments has often been used in meta-analysis models to understand ecological characteristics of marine fishes in general (Myers ref, Thorson et al. Fish and Fisheries 2015, other ideas...).  Therefore, any systematic bias in which stocks are assessed could also bias our ecological understanding of marine fishes.

3.  Stock assessments often require updates periodically (e.g., Pacific hake has been re-assessed annually from X through 2016), and agency resources might be fully expended while assessing a small fraction of stocks.  However, it is currently unknown whether the rate of assessing new species for the first time is increasing or decreasing.  If this rate is decreasing, this could indicate the need for additional public resources for stock assessment.  

We therefore seek to provide the first-ever quantitative analysis of which marine species are likely to have undergone a stock assessment using a statistical population dynamics model.  To do so, we combine two databases representing all fished marine species in the United States:  a database of all landed species from 195X to 201X, and a database of management and stock assessment attributes for all US fishes with peer-reviewed stock assessments.  For each landed stock in each of four US regions (northeast, southeast, northwest, and southwest), we record the year that it first had a stock assessment; we treat any stock that does not have an assessment by 2015 as a "censored" observation (i.e., it might eventually have a peer-reviewed assessment)  We then apply a censored time-to-event model to answer the following questions:  (1) what economic or biological characteristics are associated with a high or low annual probability of being assessed for the first time? (2) which US region has assessed stocks relatively quickly or slowly? (3) are there certain taxa (e.g., invertebrates or sharks) that are substantially faster or slower-assessed given these biological and economic attributes? and (4) is the rate of stock assessment accelerating or decelerating over time. We show that price per kilo and and landings are the main drivers of globallly increasing stock assessment rates, but some taxa defy these trends and are .

\section{Methods}

\subsection{Management database}

The year of first stock assessment for marine stocks previously assessed in the United States was determined primarily by interviews with regional stock assessment scientists, as supplemented by conducting a literature review of archived assessments. Many types of stock assessments are applied in the US, with varying levels of model complexity and input data requirements. Assessments for any given stock also tend to change over time, typically becoming more complex as warranted by available data. For consistency across populations, we defined a stock assessment as a single-species  model of density-dependent population dynamics (e.g., including some combination of individual growth, recruitment, and/or aggregate surplus production), with model parameters that were estimated by fitting to abundance index and/or age and length compositional data, and that provided time series estimates of population abundance (e.g. total biomass, spawning biomass) and/or exploitation rates (e.g., fishing mortality or harvest fractions) as well as a benchmark for these time series estimates. Benchmarks includeD target reference points, reference points based on maximum sustainable yield (MSY) or its proxies, or initial population abundance; ratios of the time series and their corresponding reference points provide a relative index of stock status. Age-structured models, delay-difference models, biomass dynamics models, and surplus production models all qualified as assessment models.  We recongize that stock-reduction analyses (SRAs) are often used to estimate overfishing limits for stocks in the absence of a population-dynamics model fitted to data. However, stock-reduction analyses did not qualify as stock assessments under this definition because they typically are not fitted to abundance-index or compositional data. Exceptions were occasionally made (i.e. definitions of an assessment were more forgiving) for some invertebrate species like squids due to short lifespans and difficulties of aging individuals.     

Year of first assessment assignments were compared with the NOAA Species Information System (SIS) database to ensure consistency. The SIS database does not contain information about when a stock was first assessed, so comparisons were restricted to recent years, assuming that stocks which qualified as assessed in a previous year continue to qualify as presently assessed. These comparisons generally showed consistency among datasets. The only exceptions... [squids, couple others].

\subsection{Defining un-assessed stocks}

Not all species landed in US fisheries are assessed using a population-dynamics model, and some are only assessed in a subset of regions. Our management database therefore only included instances of stocks with a stock assessment, and did not contain records of unassessed stocks. Excluding all un-assessed stocks would not allow us to make general inference about the propensity of stocks to be assessed. We therefore complement the management database with data from NOAA's landings database. For this, we attributed all regional landings from assessed stocks to these stocks [MIKE - I'm talking about the proportions for each stock in the spreadsheet. Can't quite recall how you did this - can you elaborate here?], and then grouped landings that were not attributed to assessed stocks into species/region combinations, which we treated as un-assessed stocks. We distinguish four regions, defined as: Alaska (i.e., the Eastern Bering Sea, Gulf of Alaska, and Aleutian Islands); US West Coast (i.e., the federal waters of Oregon, Washington, and California); Northeast Coast (including the mid-Atlantic Coast); and Southeast Coast (including the South Atlantic Coast and Gulf of Mexico). These regions do not include the portion of US federally-managed waters in the Caribbean or central Pacific, because the management database has not been consistently expanded to these regions [IS THIS CORRECT MIKE?].

This definition of "unassessed stocks" is difficult to apply in the case of highly migratory species, which often span multiple regions or cross national jurisdiction. We therefore expluded all species that are typically assessed by Regional Fisheries Management Organisations. This included all tuna, billfish and oceanic sharks. 

\subsection{Explanatory variables}

Several variables were considered as explanatory factors affecting the year in which a stock was first assessed. Region and habitat typically occupied by the population were each treated as a categorical random effect. Habitat types from FishBase (ref) or SeaLifeBase (ref) were aggregated into six categories: deep sea (>200m; bathy-pelagic or bathy-demersal); benthic; demersal; benthopelagic; pelagic; and reef-associated. Adjustments were made to the default global species-level classifications for some populations to better reflect local habitat usage. 

Maximum body length of the species was also assigned to each population and used as a numerical predictor, drawing from FishBase and SeaLifeBase. Fisheries that developed earlier may be more likely to be assessed earlier than others. The year of fishery development, defined as the year in which landings first reached 25 percent of the maximum landings across the full time series (Sethi et al. 2011? PNAS), provides historical information about the fishery for each population as was used as a numerical predictor. The catch quantity and ex-vessel price of the population together determine landed value of the population; more valuable populations may be more likely to be assessed. We considered maximum annual landings prior to the first assessment and ex-vessel price per kg in the year of first assessment as separate numerical predictors, drawn from the National Marine Fisheries Service landings database. These state-level landings and ex-vessel price data were linked to corresponding biological stock units according to areas of stock distribution as defined in assessments.


\subsection{Time-to-event model}

To assess which factors drive the over-all rate of assessments and the time from first recorded landings to a full stock assessment in US stocks, we applied a time-to-event model. Also called survival models in the medical and ecological literature, these models account for censored data (i.e., species that are landed but not yet assessed) while modeling time-to-assessment within a parametric framework.

The Weibull distribution is often used as a flexible model that has several desireable properties for this type of analysis, and one can easily check whether the Weibull model is appropriate for the data at hand (see Figure \ref{fig:Weibull_check}). The shape parameter of the Weibull density can be interpreted in terms of the rate of events occurring. A shape greater than one suggests an increasing rate of events, whereas a shape of less than one indicates an decreasing rate. This allows us to directly estimate the change in assessment rates over time.

A further desirable property is that the model can be interpreted in terms of the ratio of event rates as well as time-to-event. For example, one can interpret a model coefficient as decreasing or increasing the likelihood of an event occurring at any particular time relative to the baseline (this is usually called the hazard ratio interpretation). Thus, the rate of events may be modified by a particular variable. The model can also be interpreted in terms of time-to-event parameters, which scale the time to the event by an acceleration factor. For example, in a hyothetical scenario, the median time-to-assessment of a demersal stock may be 0.5 times that of a pelagic stock, suggesting that it takes twice as long for pelagic stocks to get assessed. Such acceleration factors are just transformations of the parmeters obtained for the event rate interpretation - the two interpretation are therefore easily exchangeable in the Weibull model.

We thus model time-to-assessment as Weibull distributed with shape parameter $\tau$ and rate $\lambda$. The connection between the event rate and the time-to-event interpretations can be made explicit by writing the Weibull density as a function of the product of the probability of the assessment occurring at time $t$ and the probability of the assessment not occurring prior to time $t$. Thus, $P(T=t)$ depends on the probability of an assessment not having occurred prior to time $t$, ($A(t) = 1-P(T\le t)=1-F(t)$), where $F(t) = \exp(-\lambda t^\tau)$ is the Weibull distribution function, and the rate $r(t)$ with which assessments occurr at time $t$. 

\begin{align}
T &\sim Weibull(\tau,\lambda) \\
  &=  A(t)\times r(t)  \\
  &= \exp(-\lambda t^\tau) \times \lambda \tau t^{\tau-1}  
\end{align}   

We modeled the scale $\lambda$ of the Weibull distribution as a function of covariates and categorical variables:

\begin{align}
log(\lambda_{i,r,h,c,o,f}) = \beta X_i + \alpha_r + \gamma_h + \kappa_c + \omega_o + \zeta_f,
\end{align}

where beta is a row-vector of regression coefficients, $X_i$ is a vector of continuous covariates. Continuous covariates were taken as the logarithm of price per kg, maximum landings, and species maximum length, standardised for the analysis. $\alpha$, $\gamma$, $\kappa$, $\omega$, and $\zeta_f$ were region, habitat, class, order, and family specific random effects. The model was estiamted within a Bayesian framework, using Markov Chain Monte Carlo (MCMC) as implemented in the JAGS package. MCMC was run using three chain of 210~000 iterations each, keeping every 100th iteration, with 10~000 iterations for each chain discarted as burn-in. This provided 6000
samples from the posterior distribution for each parameter.

All random effects were given half-Cauchy priors with a scale of $\Theta=100$, regression coeficcients had vague normal priors with a precision of $1/sigma^2 = 1e-5$, and $\tau$ was estimated using a gamma prior with parameters $a=b=1e-5$. 

\section{Results}

Across all regions of the US, the number of un-assessed fished populations increased steadily since the 1950s and into the 1990s (Figure \ref{fig:assessed_landed1}), while an increasing number of assessments in each region lead to a steadily increasing trend in the proportion of landed stocks that are assessed (Figure \ref{fig:assessed_landed2}). In terms of landings, the assessment of lage stocks in each region between the 70s and 00s lead to rapidly increasing proportions of landings being assessed. In Alaska, the vast majority of landed stocks in our database were assessed (not counting Salmon; (Figure \ref{fig:assessed_landed3}). 

Demersal species were landed more frequently than species associated with any other habitat (Figure \ref{fig:habitat}), and aslo accounted for the highest number of assessments. The majority of landed stocks were fish species (Figure \ref{fig:taxa}), with Perciformes, Pleuronectiformes and Scorpaeniformes dominating both the number of assessed and unassessed stocks.

\begin{landscape}

<<assessed_landed,fig.cap='Timeline of a) the number of stocks landed by region and assessment status, b) proportion of landed stocks that are assessed, and c) the proportion of total landings (per year) from assessed stocks. The dotted vertical line marks the enactment of the Sustainable Fisheries Act of 1996',fig.subcap=c('Number of stocks', 'Proportion of stocks', 'Proportion of landings'),echo=FALSE,results='asis',fig.width=3,fig.height=3,out.width='0.5\\textwidth',fig.align='center'>>=

l.tab <- full.tab %>% 
  group_by(mainregion,stock) %>%
  filter(year==min(year)) %>% 
  group_by(mainregion,year) %>%
  summarise(n.landed = n()) %>% 
  mutate(c.landed = cumsum(n.landed))
  
a.tab <- full.tab %>% 
  group_by(mainregion,stock) %>%
  mutate(ya=as.numeric(as.numeric(Year.of.first.stock.assessment))) %>%
  group_by(mainregion,year) %>%
  summarise(n.assessed = sum(ya==year,na.rm=T),
            land.assessed = sum(total_landings[ya<=year],na.rm=T)/1000,
            land.unassessed = sum(total_landings[ya>year|is.na(ya)],na.rm=T)/1000) %>% 
  mutate(c.assessed = cumsum(n.assessed)) 
  
plot.tab <- inner_join(l.tab, a.tab) %>% 
  mutate(c.a = ifelse(is.na(c.assessed), 0, c.assessed),
    unassessed.landed = c.landed - c.a) %>%
  select(-c.a,-c.landed,-n.assessed,-n.landed,-land.assessed,-land.unassessed) %>%
  reshape2::melt(id.vars=c('mainregion','year')) %>%
  mutate(Assessed = ifelse(variable=='unassessed.landed','No','Yes'))

plot.tab.prop <- inner_join(l.tab, a.tab) %>% 
  mutate(p.assessed = c.assessed/c.landed) 
 
land_plot <- a.tab %>% 
  mutate(p.assessed = land.assessed/(land.assessed+land.unassessed))

ggplot(plot.tab) + 
  geom_line(aes(col=mainregion,x=year,y=value,linetype = Assessed)) + 
  ylab('Number of landed stocks') + 
  xlab('Year') +
  scale_linetype('Assessed')+
  scale_colour_manual('Region',values=cbPalette) + 
  geom_vline(aes(xintercept=1996),linetype=3)+
  theme_classic()+
  theme(axis.text = element_text(color = 'black'))

ggplot(plot.tab.prop) + 
  geom_line(aes(col=mainregion,x=year,y=p.assessed)) + 
  ylab('Proportion of stocks assessed') + 
  xlab('Year') + 
  scale_linetype('Assessed')+
  scale_colour_manual('Region', values=cbPalette)+
  geom_vline(aes(xintercept=1996),linetype=3)+
  theme_classic()+
  theme(axis.text = element_text(color = 'black'))

ggplot(land_plot) + 
  geom_line(aes(col=mainregion,x=year,y=p.assessed)) + 
  ylab('Proportion of landings assessed') + 
  xlab('Year') + 
  scale_linetype('Assessed')+
  scale_colour_manual('Region', values=cbPalette)+
  geom_vline(aes(xintercept=1996),linetype=3)+
  theme_classic()+
  theme(axis.text = element_text(color = 'black'))

@

\end{landscape}


<<>>=

year.table$Assessed <- ifelse(!is.na(as.numeric(year.table$Year.of.first.stock.assessment)),'Yes','No')

@

\begin{figure}[!h]
\centering
<<habitat,fig.width=3,fig.height=3,out.width='0.5\\textwidth'>>=


simpleCap <- function(s) {
  #s <- do.call('rbind',strsplit(x, " "))
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep="")
}

yt <- year.table %>% mutate(hab = simpleCap(habitat_MM))

ggplot(yt) + 
  geom_bar(aes(fill=Assessed,x=hab)) +
  #coord_flip() + 
  theme_classic() + 
  xlab('Habitat') + 
  ylab('Count') + 
  theme(axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        axis.text = element_text(color = 'black'))
@
\caption{Assessment status at time of last known status (censoring time) by habitat}
\end{figure}

\begin{figure}[!h]
\centering
<<taxa,fig.width=6,fig.height=3,out.width='\\textwidth'>>=
yt <- year.table %>% mutate(class=substr(Class,1,1))

ggplot(yt) + 
  geom_bar(aes(fill=Assessed,x=Order), width = 1) + 
  facet_grid(~class, drop = T,switch = "both", scales = "free_x", space = "free_x") +
  #coord_flip() + 
  theme_classic() + 
  ylab('Count') + 
  theme(axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        #strip.text = element_text(angle=90,hjust = 1,vjust=0.5),
        panel.margin = unit(0, "lines"), 
        axis.text = element_text(color = 'black'),
        strip.background = element_rect())
  @
\caption{Assessment status at time of last known status (censoring time) by Order and sorted by Class. Classes are abbreviated as \Sexpr{paste(paste(unique(yt$class),unique(yt$Class),sep=': '),collapse=', ')}}
\end{figure}


\begin{table}
\centering
\small{
\caption{Posterior mean and $P(\theta>1)$ for model parameters. Parameters can be interpreted as the ratio of rates (rate effect $\theta$, i.e., rates at which stocks with different characteristics are assessed) or as multiplicative acceleration factors (time effect, i.e., $\nu=0.5$ suggests a stock with these characteristics is assessed twice as fast as the average stock).}
\begin{tabular}{lrrr}
\newline
Parameter & Rate effect ($\theta$) & Time effect ($\nu$) & $P(\theta>1)$ \\
\hline
<<table,results='asis',echo=FALSE>>=
print(xtable(data.frame(coef_P)),only.contents=TRUE, include.colnames=F, include.rownames=F,hline.after=NULL)
@
\end{tabular}
}
\end{table}

<<post_plot,fig.cap='Comparison of finite population standard deviation (i.e., variance attributed to each variable) for random effects in the Weibull survival model. The circle shows the posterior median, with thick bars showing the inter-quartile range of the posterior and the thin line is the 95\\% confidence interval',fig.width=3,fig.height=3,out.width='0.5\\textwidth',fig.align='center'>>=

fp <- get_coef_chains(model.out = a.out, coef.names = 'fp' )

.simpleCap <- function(s) {
  
  paste(toupper(substring(s, 1, 1)), substring(s, 2),
        sep = "")
}

fp$Effect <- .simpleCap(do.call('rbind',strsplit(as.character(fp$Parameter),'\\.'))[,3])
fp$Effect[fp$Effect=='Hab'] <- 'Habitat'
fp$Effect <- factor(fp$Effect, levels = rev(unique(fp$Effect)))

fp %>% group_by(Effect) %>%
  summarise(means = median(MCMC),
            q1 = quantile(MCMC,0.025),
            q3 = quantile(MCMC,0.975),
            q11 = quantile(MCMC,0.25),
            q33 = quantile(MCMC,0.75)) %>%
  ggplot() + 
  geom_point(aes(x=Effect, y=means), size=4) +
  geom_linerange(aes(x=Effect,ymin=q1,ymax=q3),size=1) +
  geom_linerange(aes(x=Effect,ymin=q11,ymax=q33),size=2) +
  ylab('Finite population SD') +
  xlab('') + 
  theme_classic() +
  coord_flip()+
  theme(axis.text = element_text(colour='black'))

@

\begin{landscape}

<<surv_plot,fig.cap='Marginal probability of a stock in category $k$ being assessed as a function of time ($P(T_k \\le t) = F_k(t) = \\exp(-\\lambda_k t^\\tau)$), for stocks of various taxonomic orders, class, regions and habitats. For taxonomic variables, only the eight levels with the most stocks represented in our dataset are shown. Marginal probabilities were evaluated at the mean of (centered) continuous covariates.',echo=FALSE,results='hide',fig.width=9,fig.height=5,out.width='1.6\\textwidth',fig.align='center'>>=

preds <- get_coef_chains(model.out = a.out, coef.names = 'pmu' )

preds$Parameter <- as.character(preds$Parameter)

preda <- preds %>% 
  split(.$Parameter) %>%
  purrr::map(function(l) {
    tp <- sapply(seq(0,50),function(t) 1-exp(-l$MCMC*t^tau$MCMC))
    data.frame(time=0:50,mm=apply(tp,2,median))
  })


mpreda <- reshape2::melt(preda,id.vars='time') %>% dplyr::select(-variable)
names(mpreda) <- c('time','MCMC','Parameter')

preda <- tbl_df(mpreda) %>% 
  mutate(effect = do.call('rbind',strsplit(as.character(Parameter),'\\.'))[,1],
         Effect = .simpleCap(effect),
         idx = as.numeric(regmatches(Parameter,regexpr('([0-9]+)',Parameter))))


preda$Effect[preda$Effect=='Hab'] <- 'Habitat'

pred.plot <- data.frame(preda) %>%
  rowwise() %>%
  mutate(Group = levels(as.factor(year.table[,unique(Effect)]))[idx])
  
p=0;gg<- list()
for(group in unique(pred.plot$Effect)){
  
  this <- names(sort(table(year.table[,group]),decreasing = T)[1:8])
  p=p+1;
  pp<- pred.plot %>% filter(Effect == group, Group %in% this)
  
  gg[[p]] <- ggplot(pp) + 
    geom_line(aes(x=time,y=MCMC,col=Group,linetype=Group)) + 
    theme_classic() + 
    theme(axis.text = element_text(colour='black'))+
    ylab('Probability of being assessed') +
    xlab('Time (yr)') + 
    scale_colour_manual(group,values=cbPalette) + 
    scale_linetype_discrete(group)+
    coord_cartesian(expand = F)
}

gridExtra::grid.arrange(grobs=gg,ncol=2)

@
\end{landscape}

\begin{landscape}
<<Effect_plot,fig.cap='Summaries of estimated posterior distributions for a) continous covariates in the model, b) habitat random effects, c) regional random effects, and d) taxonomic class random effects. The circle shows the posterior median, with thick bars showing the inter-quartile range of the posterior and the thin line is the 95\\% confidence interval.',echo=FALSE,results='hide',fig.width=8,fig.height=5,out.width='1.5\\textwidth',fig.align='center'>>=

betas <- get_coef_chains(model.out = a.out, coef.names = 'betas\\[[0-9]*\\]', var.names = c('Length','Maximum landings', 'Mean price per kg'))
betas$Effect <- 'Covariates'

habitats <- get_coef_chains(model.out = a.out, coef.names = 'habitat\\[[0-9]*\\]', var.names = levels(as.factor(year.table$habitat_MM)))
habitats$Effect <- 'Habitat'
habitats$Parameter <- as.character(habitats$Parameter)
habitats$Parameter <- simpleCap(habitats$Parameter)

regions <- get_coef_chains(model.out = a.out, coef.names = 'region\\[[0-9]*\\]', var.names = levels(as.factor(year.table$mainregion)))
regions$Effect <- 'Region'

classes <- get_coef_chains(model.out = a.out, coef.names = 'classfx\\[[0-9]*\\]', var.names = levels(as.factor(year.table$Class)))
classes$Effect <- 'Class'


fx.plot <- rbind(betas,habitats,regions,classes)

fx.plot %>%
  group_by(Effect,Parameter) %>%
  summarise(means = (median(MCMC)),
            q1 = (quantile(MCMC,0.025)),
            q3 = (quantile(MCMC,0.975)),
            q11 = (quantile(MCMC,0.25)),
            q33 = (quantile(MCMC,0.75))) %>%
  
  ggplot() + 
  geom_point(aes(x=Parameter, y=means), size=4) +
  geom_linerange(aes(x=Parameter,ymin=q1,ymax=q3),size=1) +
  geom_linerange(aes(x=Parameter,ymin=q11,ymax=q33),size=2) +
  ylab('') +
  facet_wrap(~Effect,drop = T, scales = "free",as.table = F) +
  xlab('') +
  geom_hline(aes(yintercept=0), linetype=2) + 
  theme_classic() +
  coord_flip()+
  theme(axis.text = element_text(colour='black'))

#gridExtra::grid.arrange(grobs=gg,nrow=1)
@


\end{landscape}

<<Effect_plot_oder,fig.cap='Summaries of estimated posterior distributions for order within class, showing a summary (grey line: posterior mean, coloured box: 95\\% confidence) of class effects, and order effects relative to class effects (points: posterior mean, black line: 95\\% confidence), for classes with multiple orders in the dataset.',echo=FALSE,results='hide',fig.width=6,fig.height=3,out.width='1\\textwidth',fig.align='center'>>=

afs <- function(x) as.numeric(as.factor(x))
orders <- with(year.table,afs(Order))
class <- with(year.table,afs(Class))
classord <- tapply(class,orders,unique)


cl <- classes %>% group_by(Parameter) %>% mutate(iter=1:n()) %>% tidyr::spread(Parameter,MCMC)

orderr <- get_coef_chains(model.out = a.out, coef.names = 'orderfx\\[[0-9]*\\]', var.names = levels(as.factor(year.table$Order)))
orderr$Effect <- 'Order'
orderr$class <- levels(as.factor(year.table$Class))[classord[match(orderr$Parameter,levels(as.factor(year.table$Order)))]]

orderr[,c('Class','Iter')] <- reshape2::melt(cl[,-c(1,2)][,classord])

orderr$Class <- as.character(orderr$Class)

orderr %>%
  group_by(Parameter,Class) %>%
  summarise(omeans = mean(MCMC+Iter),
            cmeans = mean(Iter),
            oq1 = quantile(MCMC+Iter,0.025),
            cq1 = quantile(Iter,0.025),
            oq3 = quantile(MCMC+Iter,0.975),
            cq3 = quantile(Iter,0.975),
            oq11 = quantile(MCMC+Iter,0.25),
            cq11 = quantile(Iter,0.25),
            oq33 = quantile(MCMC+Iter,0.75),
            cq33 = quantile(Iter,0.75)) %>%
  group_by(Class) %>%
  mutate(ns = n()) %>% 
  filter(ns>1) %>%
  ggplot() + 
  geom_crossbar(aes(x=Parameter, y=cmeans,ymin=cq1,ymax=cq3,fill=Class),col='grey50',alpha=0.5,size=0.01,fatten=100) +
  facet_grid(~Class, drop = T,switch = "both", scales = "free_x", space = "free_x") +
  scale_fill_manual(values=cbPalette) + 
  geom_point(aes(x=Parameter, y=omeans), size=1) +
  geom_linerange(aes(x=Parameter,ymin=oq1,ymax=oq3),size=1) +
  #geom_linerange(aes(x=Parameter,ymin=oq11,ymax=oq33),size=2) +
  ylab('') +
  xlab('') +
  #geom_hline(aes(yintercept=0), linetype=2) + 
  theme_classic() +
  #coord_flip()+
  theme(axis.text = element_text(colour='black'),
        axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        #strip.text = element_text(angle=90,hjust = 1,vjust=0.5),
        panel.spacing = unit(0, "lines"), 
        strip.background = element_rect(),
        strip.text = element_blank())

@

<<surv_plot_region,fig.cap='Projected proportion of stocks assessed by region and calendar year, based on assessment probabilities of stocks within each region over the projected year range.',echo=FALSE,results='hide',fig.width=5,fig.height=3,out.width='1\\textwidth',fig.align='center'>>=

devtime <- apply(cbind(year.table$Year.of.fishery.development..stock.based.,year.table$minyear),1,min,na.rm=T)

a.time <- as.numeric(year.table$Year.of.first.stock.assessment) - devtime

# true false censoring
censored <- as.logical(is.na(a.time))

preds <- get_coef_chains(model.out = a.out, coef.names = '(mu\\[[0-9]*)')

mus <- tbl_df(preds) %>% filter(!grepl('\\.',Parameter))

mus$Parameter <- as.character(mus$Parameter)
mus$ymin <- 2016-rep(year.table$minyear,each=6000)
mus$cc <- rep(censored,each=6000)
mus$Region <- rep(year.table$mainregion,each=6000)

mus.p <- mus %>% 
  split(.$Parameter) %>%
  purrr::map(function(l) {
    lmin <- unique(l$ymin)
    tp <- sapply(seq(lmin,lmin+34),function(t) 1-exp(-l$MCMC*t^tau$MCMC))
    data.frame(time=2016:2050,mm=apply(tp,2,median),mq1=apply(tp,2,quantile,0.025),mq2=apply(tp,2,quantile,0.975),censored=unique(l$cc),Region=unique(l$Region))
  })


mpreda <- reshape2::melt(mus.p,id.vars=c('time','censored','Region'))
names(mpreda) <- c('time','censored','Region','Quantile','MCMC','Parameter')


preda <- tbl_df(mpreda) %>%
  group_by(Region, time, Quantile) %>%
  summarise(pro_assessed = mean(!censored),
            pro_pred = pro_assessed+(1-pro_assessed)*mean(MCMC[censored==T])) %>%
  tidyr::spread(key = Quantile,value = pro_pred)
  
ggplot(preda) + 
 geom_line(aes(x=time,y=mm)) + 
  geom_line(aes(x=time,y=mq1), linetype=2) +
  geom_line(aes(x=time,y=mq2), linetype=2) + 
  geom_ribbon(aes(x=time,y=mm,ymin=mq1,ymax=mq2),alpha=0.4,col=NA) + 
  theme_classic() + 
  facet_wrap(~Region) + 
  theme(axis.text = element_text(colour='black'))+
  ylab('Proportion assessed') +
  xlab('Calendar year') + 
  #scale_colour_manual(values=cbPalette) + 
  #scale_fill_manual(values=cbPalette) + 
    coord_cartesian(expand = F, ylim=c(0,1))


@

\FloatBarrier
\newpage
\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}   
\setcounter{figure}{0} 
\section{A: Model fit}
\begin{figure}[!ht]
\centering

<<fig.width=4,fig.height=4,out.width='0.7\\textwidth'>>=

#assessment time


# Kaplan-Meyer non-parametric survival at t - should be linear with slope p
km.cs <- survfit(Surv(a.time,!censored) ~ 1)
summary.km.cs <- summary(km.cs)
rcu <- summary.km.cs$time 
surv.cs <- summary.km.cs$surv
plot(log(rcu),log(-log(surv.cs)),type="p",pch=16,xlab="log(t)",ylab="log(-log(S(t)))")
lm_fit <- lm(log(-log(surv.cs+1e-10))~log(rcu))$coefficients

abline(a=lm_fit[1],b=lm_fit[2],col=3,lwd=2); 
@
\caption{Appropriateness of the Weibull event-time model for the time-to-assessment dataset. If the Weibull applies, the time from fishery development to assessment should fall on a line with slope $p$ (the Weibull shape parameter) between $log(-log(\hat{S}(t)))$, where $\hat{S}(t)$ is the non-parametric Kaplan-Meyer estimate of survival at time $t$, and the log of $t$. Here, $p$ evaluates to \Sexpr{round(lm_fit[2],2)}, suggesting an increasing assessment rate with increasing time $t$.}
\label{fig:Weibull_check}
\end{figure}

\begin{figure}[!ht]
\centering
<<fig.width=4,fig.height=4,out.width='0.7\\textwidth'>>=

# Kaplan-Meyer non-parametric survival at CS - should follow exp(1) distribution
CS.full <- tbl_df(get_coef_chains(model.out = a.out, coef.names = 'CS'))

# just look at mean CS for now, can put posterior around it later
CS.means <- CS.full %>%
  group_by(Parameter) %>%
  summarise(post.mean = mean(MCMC))

CS = CS.means$post.mean

devtime <- apply(cbind(year.table$Year.of.fishery.development..stock.based.,year.table$minyear),1,min,na.rm=T)

a.time <- as.numeric(year.table$Year.of.first.stock.assessment) - devtime
censored <- as.logical(is.na(a.time))

km.cs <- survfit(Surv(CS,!censored) ~ 1)
summary.km.cs <- summary(km.cs)
rcu <- summary.km.cs$time # Cox-Snell residuals of
                            # uncensored points.
surv.cs <- summary.km.cs$surv
plot(rcu,-log(surv.cs),type="p",pch=16,
xlab="Cox-Snell residual",ylab="Cumulative hazard")
abline(a=0,b=1,col=3,lwd=2) 

@
\caption{Model fit of the Weibull survival model, based on Cox-Snell residuals calculated at the posterior mean of the linear predictor. For a perfect fit all data points (solid points) would lie on the y=x line.}
\end{figure}


<<>>=

orderr <- get_coef_chains(model.out = a.out, coef.names = 'orderfx\\[[0-9]*\\]', var.names = levels(as.factor(year.table$Order)))
orderr$Effect <- 'Order'


fx.tab <- rbind(betas,habitats,regions,classes,orderr) 
fx.tab$Rate <- tau$MCMC

fx.tab.sum <- fx.tab %>%
  group_by(Effect, Parameter) %>%
  summarise(m = median(exp(MCMC)),
            acc = median(exp(-MCMC/Rate)),
            p = mean(MCMC>0))

ptab <- order(match(fx.tab.sum$Effect, c('Covariates','Region','Habitat','Class','Order')))


@

\FloatBarrier
\newpage
\begin{center}
\begin{longtable}{llrrr}
\caption[Paramter estimates]{Parameter estimates and probability $P(\theta_k>0)$, the probability that stocks in this category $k$ have a higher than average rate of assessment. Parameters can be interpreted as the ratio of rates between a stock of category $k$ and an average stock (rate effect $\theta$) or as multiplicative acceleration factors (time effect, e.g., $\nu=0.5$ suggests a stock with these characteristics is assessed twice as fast as the average stock).} 
\label{tab:fx} \\

Effect & Category & Rate effect ($\theta$) & Time effect ($\nu$) & $P(\theta>1)$ \\
\addlinespace
\endfirsthead

\multicolumn{5}{c}%
{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\
\hline
\addlinespace 
Effect & Category & Rate effect ($\theta$) & Time effect ($\nu$) & $P(\theta>1)$ \\
\addlinespace 
\endhead

\addlinespace 
\hline 
\addlinespace 
\multicolumn{5}{r}{Continued on next page} \\
\endfoot

\endlastfoot

<<results="asis">>=
print(xtable(fx.tab.sum[ptab,]),
      hline.after = -1,
      only.contents = T,
      include.rownames = F, 
      include.colnames = F)
@
\end{longtable}
\end{center}

\end{document}