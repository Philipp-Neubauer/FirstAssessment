\documentclass{article}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{amsmath}
\usepackage{pdflscape}
\usepackage[toc,page]{appendix}
\begin{document}

\title{Are assessed stocks a representative sample of fished species in the US?}

\author{Philipp Neubauer, Dragonfly Data Science, Wellington, NZ \\
\and James T. Thorsen, NOAA Northwest Fisheries Science Center, Seattle 
\and            Michael C. Melnychuk, School of Aquatic and Fisheries Science, \\University of Washington, Seattle}

\maketitle


<<echo=FALSE>>=
require(knitr)
opts_chunk$set(warning=F, message = FALSE,echo=F,error=FALSE,cache=F, autodep=TRUE)
@

<<preamble,results='hide'>>=
DB <- '~/Dropbox'

# Settings for Jim's machine
if( Sys.info()["effective_user"] == "James.Thorson" ){
  setwd("C:/Users/James.Thorson/Desktop/Project_git/FirstAssessment")
  DB <- 'C:/Users/James.Thorson/Dropbox'
}

source('helper_functs.R')
require("dplyr")
require(xtable)
require("ggplot2")
library(survival)

load(file.path(DB,"First year of assessment/Weibull_model_output.rda"),v=T)

# just for matching
year.table$Region <- year.table$mainregion
year.table$Habitat <- year.table$habitat_MM
year.table$region <- year.table$mainregion
year.table$habitat <- year.table$habitat_MM

coeffs <- tbl_df(get_coef_chains(model.out = a.out, 
                                 coef.names = 'betas',
                                 var.names = c('Length',
                                               'Maximum landings',
                                               'Mean price per kg')))


tau <- tbl_df(get_coef_chains(model.out = a.out, 
                                 coef.names = 'tau',
                                 var.names = 'Rate'))
coeffs$Rate = tau$MCMC
# regressin coeffs are beta
coef_P <- coeffs %>%
  group_by(Parameter) %>%
  summarise(post.mean = mean(exp(MCMC)),
            acc = mean(exp(-MCMC/Rate)),
            post.P = mean(exp(MCMC) > 1))
@

\FloatBarrier
\newpage
\section{Introduction}

It is often said "what gets measured gets managed".  For over XXXX years (historical citation), fisheries scientists have measured
human impacts on ocean populations (finfishes and marine invertebrates) with the goal of balancing both the long-term sustainability
of human impacts with maximizing harvest (or value) from fishing.  In particular, stock assessments generally involve
estimating two measures of human impact: (1) fishing rate, i.e., the instantaneous mortality or annual fraction of the population that is harvested relative to an estimated target level, and (2) the population abundance, i.e., spawning biomass or reproductive output relative to an estimated target level.  The value of these two metrics can be plotted on a 2-dimensional "stock status" plot, and fisheries agencies are increasingly committed to maintaining fished populations in the "healthy" quadrant (i.e., fishing rate below a target level, and population abundance above a target level; citation).

Recently, the National Marine Fisheries Service (NMFS, the agency in charge of science supporting fisheries management in the United States) has committed to "end overfishing" for all marine species within regional fisheries management plans (with exceptions granted in a few circumstances; citation). In the US, overfishing is defined as any stock having annual harvest above limit levels. A target (or limit) harvest can in theory be calculated by combining a target harvest rate with a target population abundance.  However, the vast majority of overfishing limits are currently estimated using methods that do not individually calculate either harvest rate or population abundance (Berkson and Thorson 2015, Newman Berkson Sautoni 2015).  For example, depletion-corrected average catch (DCAC; MacCall 2009) is used to calculate an annual fishing limit for many stocks, but is not used to calculate population abundance. DCAC (and similar methods) can therefore be used to help "end overfishing" but is not otherwise informative about characteristics of the fished population. 

However, conservationists and ecologists will often be more interested in estimating population abundance (or abundance relative to equilibrium conditions) than estimating an overfishing limit (citations).  Estimating abundance generally requires applying a population model to available harvest data and an index of population depletion (either an index proportional to population abundance, or average size/age data);  in these cases, the population model essentially treats the historical fishing as a "depletion experiment" to estimate the fraction of the population that is fished every year.  In the following, we call these population models "stock assessments", although we acknowledge that other authors have used the term "stock assessment" more broadly to also include methods for estimating overfishing limits (e.g., DCAC). Although NMFS has estimated overfishing limits for the vast majority of fishes in US fisheries management plans, a much smaller percentage of fished species have a stock assessment (in our sense). This relative dearth of stock assessment models presumably arises because developing a stock assessment is time-consuming and requires extensive financial resources from NMFS and other interested parties (Geremont ref).

We  argue that stock assessments (population models for estimating absolute abundance for fished species) are important for many applied and theoretical questions regarding marine ecosystems.  However, there is little previous research regarding which fished species are more or less likely to receive sufficient attention to develop a stock assessment.  Understanding which species are more or less likely to be assessed could be useful for the following three reasons (among others):

1.  Unassessed stocks may receive less attention from the public or fisheries managers regarding management actions. Therefore, if a given taxon is less likely to be assessed, it might also be less likely to receive rapid or public attention when management changes are warrented.

2.  Output from stock assessments has often been used in meta-analysis models to understand ecological characteristics of marine fishes in general (Myers ref, Thorson et al. Fish and Fisheries 2015, other ideas...).  Therefore, any systematic bias in which stocks are assessed could also bias our ecological understanding of marine fishes.

3.  Stock assessments often require updates periodically (e.g., Pacific hake has been re-assessed annually from X through 2016), and agency resources might be fully expended while assessing a small fraction of stocks.  However, it is currently unknown whether the rate of assessing new species for the first time is increasing or decreasing.  If this rate is decreasing, this could indicate the need for additional public resources for stock assessment.  

We therefore seek to provide the first-ever quantitative analysis of which marine species are likely to have undergone a stock assessment.  To do so, we combine two databases representing all fished marine species in the United States:  a database of all landed species from 195X to 201X, and a database of management and stock assessment attributes for all US fishes with peer-reviewed stock assessments.  For each landed stock in each of four US regions (northeast, southeast, northwest, and southwest), we record the year that it first had a stock assessment; we treat any stock that does not have an assessment by 2015 as a "censored" observation (i.e., it might eventually have a peer-reviewed assessment)  We then apply a censored time-to-event model to answer the following questions:  (1) what economic or biological characteristics are associated with a high or low annual probability of being assessed for the first time? (2) which US region has assessed stocks relatively quickly or slowly? (3) are there certain taxa (e.g., invertebrates or sharks) that are substantially faster or slower-assessed given these biological and economic attributes? and (4) is the rate of stock assessment accelerating or decelerating over time.  [Add one sentence high-line conclusion -- I know some people don't do this, but I think its a reasonable way to hook interest]

\section{Methods}

\subsection{Management database}

The year of first stock assessment was determined primarily by interviews with regional stock assessment scientists, or occasionally by searching through archived assessments. In reality numerous types of stock assessments exist, with varying levels of model complexity and input data requirements. Assessments for any given stock also tend to change over time, typically becoming more complex as warranted by available data. For consistency across populations, we required a stock assessment in some year to be based on a single-species population model which provided time series estimates of some form of abundance (e.g. total biomass, spawning biomass) and/or fishing mortality (or exploitation rate) as well as some type of benchmark for these time series estimates. Benchmarks could include target reference points, reference points based on maximum sustainable yield (MSY) or its proxies, or initial population abundance; ratios of the time series and their corresponding reference points provide a relative index of stock status. Age-structured models, delay-difference models, biomass dynamics models, and surplus production models all qualified as population models. Exceptions were occasionally made (i.e. definitions of an assessment were more forgiving) for some invertebrate species like squids due to short lifespans and difficulties of aging individuals.

Several variables were considered as explanatory factors affecting the year in which a stock was first assessed. Regional differences across the United States may arise. Populations were classified into four regions, which were treated as categorical random effects: Alaska; West Coast; Northeast Coast (including the mid-Atlantic Coast); and Southeast Coast (including the South Atlantic Coast and Gulf of Mexico). The habitat typically occupied by the population was also considered as a categorical random effect. Habitat types from FishBase (ref) or SeaLifeBase (ref) were aggregated into four (five?) categories (benthic; demersal; benthopelagic(?); pelagic; and reef-associated), and adjustments were made to the default global species-level classifications for some populations to better reflect local habitat usage. Maximum body length of the species was also assigned to each population and used as a numerical predictor, drawing from FishBase and SeaLifeBase. Fisheries that developed earlier may be more likely to be assessed earlier than others. The year of fishery development, defined as the year in which landings first reached 25 percent of the maximum landings across the full time series (Sethi et al. 2011? PNAS), provides historical information about the fishery for each population as was used as a numerical predictor. The catch quantity and ex-vessel price of the population together determine landed value of the population; more valuable populations may be more likely to be assessed. We considered maximum annual landings prior to the first assessment and ex-vessel price in the year of first assessment as separate numerical predictors, drawn from the National Marine Fisheries Service landings database. These state-level landings and ex-vessel price data were linked to corresponding biological stock units according to areas of stock distribution as defined in assessments.

Year of first assessment assignments were compared with the NOAA Species Information System (SIS) database to ensure consistency. The SIS database does not contain information about when a stock was first assessed, so comparisons were restricted to recent years, assuming that stocks which qualified as assessed in a previous year continue to qualify as presently assessed. These comparisons generally showed consistency among datasets. The only exceptions... [squids, couple others].

\subsection{Landings data}

\subsection{Data preparation}

\subsection{Time-to-event model}

To assess which factors drive the over-all rate of assessments and the time from first recorded landings to a full stock assessment in US stocks, we applied a time-to-event model. Also called survival models in the medical and ecological literature, these models account for censored data (i.e., species that are landed but not yet assessed) while modeling time-to-assessment within a parametric framework.

The Weibull distribution is often used as a flexible model that has several desireable properties for this type of analysis, and one can easily check whether the Weibull model is appropriate for the data at hand (see Figure \ref{fig:Weibull_check}). The rate parameter of the Weibull can be interpreted in terms of the rate of events occurring. A slope greater than one suggests an increasing rate of events, whereas a slope of less than one indicates an decreasing rate. This allows us to directly estimate the change in assessment rates over time.

A further desirable property is the model can be interpreted in terms of the ratio of event rates as well as time-to-event. For example, one can interpret a model coefficient as decreasing or increasing the likelihood of an event occurring at any particular time relative to the baseline (this is usually called the hazard ratio interpretation). Thus, the rate of events may be modified by a particular variable. The model can also be interpreted in terms of time-to-event parameters, which scale the time to the event by an acceleration factor. For example, the median time-to-assessment of a demersal stock may be 0.5 times that of a pelagic stock, suggesting that it takes twice as long for pelagic stocks to get assessed. Such acceleration factors are just transformations of the parmeters obtained for the event rate interpretation - the two interpretation are therefore easily exchangeable in the Weibull model.

We thus model time-to-assessment as Weibull distributed with shape parameter $\tau$ and rate $\lambda$. The connection between the event rate and the time-to-event interpretations can be made explicit by writing the Weibull density as a function of the product of the probability of an event occurring at time $t$ and the probability of an event not occurring prior to time $t$. Thus, $P(T=t)$ depends on the probability of that event not having occurred prior to time $t$ and the probability of the event occurring at time $t$. Thus,

\begin{align}
P(T=t) &= Weibull(\tau,\lambda) \\
       &= r(t) \times A(t) \\
       &= \lambda \tau t^{\tau-1} \times \exp(-\lambda t^\tau)
\end{align}


\section{Results}

\begin{landscape}

<<assessed_landed,fig.cap='Timeline of a) the number of stocks landed by region and assessment status and b) proportion of landed stocks that are assessed. The vertical line marks the enactment of the Sustainable Fisheries Act of 1996',fig.subcap=c('Number of stocks', 'Proportion of stocks'),echo=FALSE,results='asis',fig.width=4,fig.height=4,out.width='0.7\\textwidth',fig.align='center'>>=

l.tab <- full.tab %>% 
  group_by(mainregion,stock) %>%
  filter(year==min(year)) %>% 
  group_by(mainregion,year) %>%
  summarise(n.landed = n()) %>% 
  mutate(c.landed = cumsum(n.landed))
  
a.tab <- full.tab %>% 
  group_by(mainregion,stock) %>%
  mutate(ya=as.numeric(as.numeric(Year.of.first.stock.assessment))) %>%
  group_by(mainregion,year) %>%
  summarise(n.assessed = sum(ya==year,na.rm=T)) %>% 
  mutate(c.assessed = cumsum(n.assessed)) 
  

plot.tab <- inner_join(l.tab, a.tab) %>% 
  mutate(c.a = ifelse(is.na(c.assessed), 0, c.assessed),
    unassessed.landed = c.landed - c.a) %>%
  select(-c.a,-c.landed,-n.assessed,-n.landed) %>%
  reshape2::melt(id.vars=c('mainregion','year')) %>%
  mutate(Assessed = ifelse(variable=='unassessed.landed','No','Yes'))
  
  

plot.tab.prop <- inner_join(l.tab, a.tab) %>% 
  mutate(p.assessed = c.assessed/c.landed) 
 

ggplot(plot.tab) + 
  geom_line(aes(col=mainregion,x=year,y=value,linetype = Assessed)) + 
  ylab('Number of landed stocks') + 
  xlab('Year') +
  scale_linetype('Assessed')+
  viridis::scale_color_viridis('Region',discrete = T) + 
  geom_vline(aes(xintercept=1996))+
  theme_classic()+
  theme(axis.text = element_text(color = 'black'))

ggplot(plot.tab.prop) + 
  geom_line(aes(col=mainregion,x=year,y=p.assessed)) + 
  ylab('Proportion assessed') + 
  xlab('Year') + 
  scale_linetype('Assessed')+
  viridis::scale_color_viridis('Region', discrete = T)+
  geom_vline(aes(xintercept=1996))+
  theme_classic()+
  theme(axis.text = element_text(color = 'black'))
@

\end{landscape}


<<>>=

year.table$Assessed <- ifelse(!is.na(as.numeric(year.table$Year.of.first.stock.assessment)),'Yes','No')

@

\begin{figure}[!h]
\centering
<<fig.width=4,fig.height=4,out.width='0.7\\textwidth'>>=


simpleCap <- function(s) {
  #s <- do.call('rbind',strsplit(x, " "))
  paste(toupper(substring(s, 1,1)), substring(s, 2),
      sep="")
}

yt <- year.table %>% mutate(hab = simpleCap(habitat_MM))

ggplot(yt) + 
  geom_bar(aes(fill=Assessed,x=hab)) +
  #coord_flip() + 
  theme_classic() + 
  xlab('Habitat') + 
  ylab('Count') + 
  theme(axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        axis.text = element_text(color = 'black'))
@
\caption{Assessment status at time of last known status (censoring time) by habitat}
\end{figure}

\begin{figure}[!h]
\centering
<<fig.width=5,fig.height=5,out.width='\\textwidth'>>=
yt <- year.table %>% mutate(class=substr(Class,1,1))

ggplot(yt) + 
  geom_bar(aes(fill=Assessed,x=Order), width = 1) + 
  facet_grid(~class, drop = T,switch = "both", scales = "free_x", space = "free_x") +
  #coord_flip() + 
  theme_classic() + 
  ylab('Count') + 
  theme(axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        #strip.text = element_text(angle=90,hjust = 1,vjust=0.5),
        panel.margin = unit(0, "lines"), 
        axis.text = element_text(color = 'black'),
        strip.background = element_rect())
  @
\caption{Assessment status at time of last known status (censoring time) by Order and sorted by Class. Classes are abbreviated as \Sexpr{paste(paste(unique(yt$class),unique(yt$Class),sep=': '),collapse=', ')}}
\end{figure}


\begin{table}
\centering
\small{
\caption{Posterior mean and $P(\beta>1)$ for model parameters. Parameters can be interpreted as the ration of rates (rate effect, i.e., rates at which stocks with different characteristics are assessed) or as multiplicative acceleration factors (time effect, i.e., $\alpha$=0.5 suggests a stock with these characteristics is assessed twice as fast as the average stock).}
\begin{tabular}{lrrr}
\newline
Parameter & Rate effect ($\beta$) & Time effect ($\alpha=\beta\tau^-1$) & $P(\beta>1)$\\
\hline
<<table,results='asis',echo=FALSE>>=
print(xtable(data.frame(coef_P)),only.contents=TRUE, include.colnames=F, include.rownames=F,hline.after=NULL)
@
\end{tabular}
}
\end{table}

<<post_plot,fig.cap='Comparison of finite population standard deviation for random effects in the Weibull survival model. The circle shows the posterior median, with thick bars showing the inter-quartile range of the posterior and the thin line is the 95\\% confidence interval',fig.width=4,fig.height=4,out.width='0.7\\textwidth',fig.align='center'>>=

fp <- get_coef_chains(model.out = a.out, coef.names = 'fp' )

.simpleCap <- function(s) {
  
  paste(toupper(substring(s, 1, 1)), substring(s, 2),
        sep = "")
}

fp$Effect <- .simpleCap(do.call('rbind',strsplit(as.character(fp$Parameter),'\\.'))[,3])
fp$Effect[fp$Effect=='Hab'] <- 'Habitat'
fp$Effect <- factor(fp$Effect, levels = rev(unique(fp$Effect)))

fp %>% group_by(Effect) %>%
  summarise(means = median(MCMC),
            q1 = quantile(MCMC,0.025),
            q3 = quantile(MCMC,0.975),
            q11 = quantile(MCMC,0.25),
            q33 = quantile(MCMC,0.75)) %>%
  ggplot() + 
  geom_point(aes(x=Effect, y=means), size=4) +
  geom_linerange(aes(x=Effect,ymin=q1,ymax=q3),size=1) +
  geom_linerange(aes(x=Effect,ymin=q11,ymax=q33),size=2) +
  ylab('Finite populaition SD') +
  xlab('') + 
  theme_classic() +
  coord_flip()+
  theme(axis.text = element_text(colour='black'))

@

\begin{landscape}

<<surv_plot,fig.cap='Marginal probability of a stock being assessed as a function of time, for stocks of various taxonomic orders, class, regions and habitats. For taxonomic variables, only the eight levels with the most stocks represented in our dataset are shown. Marginal probabilities were evaluated at the mean of continuous covariates.',echo=FALSE,results='hide',fig.width=9,fig.height=5,out.width='1.4\\textwidth',fig.align='center'>>=

preds <- get_coef_chains(model.out = a.out, coef.names = 'pmu' )

preds$Parameter <- as.character(preds$Parameter)

preda <- preds %>% 
  split(.$Parameter) %>%
  purrr::map(function(l) {
    tp <- sapply(seq(0,50),function(t) 1-exp(-l$MCMC*t^tau$MCMC))
    data.frame(time=0:50,mm=apply(tp,2,median))
  })


mpreda <- reshape2::melt(preda,id.vars='time') %>% dplyr::select(-variable)
names(mpreda) <- c('time','MCMC','Parameter')

preda <- tbl_df(mpreda) %>% 
  mutate(effect = do.call('rbind',strsplit(as.character(Parameter),'\\.'))[,1],
         Effect = .simpleCap(effect),
         idx = as.numeric(regmatches(Parameter,regexpr('([0-9]+)',Parameter))))


preda$Effect[preda$Effect=='Hab'] <- 'Habitat'

pred.plot <- data.frame(preda) %>%
  rowwise() %>%
  mutate(Group = levels(as.factor(year.table[,unique(Effect)]))[idx])
  
p=0;gg<- list()
for(group in unique(pred.plot$Effect)){
  
  this <- names(sort(table(year.table[,group]),decreasing = T)[1:8])
  p=p+1;
  pp<- pred.plot %>% filter(Effect == group, Group %in% this)
  
  gg[[p]] <- ggplot(pp) + 
    geom_line(aes(x=time,y=MCMC,col=Group,linetype=Group)) + 
    theme_classic() + 
    theme(axis.text = element_text(colour='black'))+
    ylab('Probability of being assessed') +
    xlab('Time (yr)') + 
    viridis::scale_color_viridis(group,discrete = T) + 
    scale_linetype_discrete(group)+
    coord_cartesian(expand = F)
}

gridExtra::grid.arrange(grobs=gg,ncol=2)

@
\end{landscape}

\begin{landscape}
<<Effect_plot,fig.cap='Summaries of estimated posterior distributions for a) continous covariates in the model, b) habitat random effects, c) regional random effects, and d) taxonomic class random effects. The circle shows the posterior median, with thick bars showing the inter-quartile range of the posterior and the thin line is the 95\\% confidence interval.',echo=FALSE,results='hide',fig.width=8,fig.height=5,out.width='1.2\\textwidth',fig.align='center'>>=

betas <- get_coef_chains(model.out = a.out, coef.names = 'betas\\[[0-9]*\\]', var.names = c('Length','Maximum landings', 'Mean price per kg'))
betas$Effect <- 'Covariates'

habitats <- get_coef_chains(model.out = a.out, coef.names = 'habitat\\[[0-9]*\\]', var.names = levels(as.factor(year.table$habitat_MM)))
habitats$Effect <- 'Habitat'

regions <- get_coef_chains(model.out = a.out, coef.names = 'region\\[[0-9]*\\]', var.names = levels(as.factor(year.table$mainregion)))
regions$Effect <- 'Region'

classes <- get_coef_chains(model.out = a.out, coef.names = 'classfx\\[[0-9]*\\]', var.names = levels(as.factor(year.table$Class)))
classes$Effect <- 'Class'


fx.plot <- rbind(betas,habitats,regions,classes)

fx.plot %>%
  group_by(Effect,Parameter) %>%
  summarise(means = (median(MCMC)),
            q1 = (quantile(MCMC,0.025)),
            q3 = (quantile(MCMC,0.975)),
            q11 = (quantile(MCMC,0.25)),
            q33 = (quantile(MCMC,0.75))) %>%
  
  ggplot() + 
  geom_point(aes(x=Parameter, y=means), size=4) +
  geom_linerange(aes(x=Parameter,ymin=q1,ymax=q3),size=1) +
  geom_linerange(aes(x=Parameter,ymin=q11,ymax=q33),size=2) +
  ylab('') +
  facet_wrap(~Effect,drop = T, scales = "free",as.table = F) +
  xlab('') +
  geom_hline(aes(yintercept=0), linetype=2) + 
  theme_classic() +
  coord_flip()+
  theme(axis.text = element_text(colour='black'))

#gridExtra::grid.arrange(grobs=gg,nrow=1)
@


\end{landscape}

<<Effect_plot_oder,fig.cap='Summaries of estimated posterior distributions for order within class, showing a summary (grey line: posterior mean, coloured box: 95\\% confidence) of class effects, and order effects relative to class effects (points: posterior mean, black line: 95\\% confidence).',echo=FALSE,results='hide',fig.width=5,fig.height=3,out.width='1\\textwidth',fig.align='center'>>=

afs <- function(x) as.numeric(as.factor(x))
orders <- with(year.table,afs(Order))
class <- with(year.table,afs(Class))
classord <- tapply(class,orders,unique)


cl <- classes %>% group_by(Parameter) %>% mutate(iter=1:n()) %>% tidyr::spread(Parameter,MCMC)

orderr <- get_coef_chains(model.out = a.out, coef.names = 'orderfx\\[[0-9]*\\]', var.names = levels(as.factor(year.table$Order)))
orderr$Effect <- 'Order'
orderr$class <- levels(as.factor(year.table$Class))[classord[match(orderr$Parameter,levels(as.factor(year.table$Order)))]]

orderr[,c('Class','Iter')] <- reshape2::melt(cl[,-c(1,2)][,classord])

orderr$Class <- as.character(orderr$Class)

orderr %>%
  group_by(Parameter,Class) %>%
  summarise(omeans = mean(MCMC+Iter),
            cmeans = mean(Iter),
            oq1 = quantile(MCMC+Iter,0.025),
            cq1 = quantile(Iter,0.025),
            oq3 = quantile(MCMC+Iter,0.975),
            cq3 = quantile(Iter,0.975),
            oq11 = quantile(MCMC+Iter,0.25),
            cq11 = quantile(Iter,0.25),
            oq33 = quantile(MCMC+Iter,0.75),
            cq33 = quantile(Iter,0.75)) %>%
  group_by(Class) %>%
  mutate(ns = n()) %>% 
  filter(ns>1) %>%
  ggplot() + 
  geom_crossbar(aes(x=Parameter, y=cmeans,ymin=cq1,ymax=cq3,fill=Class),col='grey50',alpha=0.5,size=0.01,fatten=100) +
  facet_grid(~Class, drop = T,switch = "both", scales = "free_x", space = "free_x") +
  viridis::scale_fill_viridis(discrete = T) + 
  geom_point(aes(x=Parameter, y=omeans), size=1) +
  geom_linerange(aes(x=Parameter,ymin=oq1,ymax=oq3),size=1) +
  #geom_linerange(aes(x=Parameter,ymin=oq11,ymax=oq33),size=2) +
  ylab('') +
  xlab('') +
  #geom_hline(aes(yintercept=0), linetype=2) + 
  theme_classic() +
  #coord_flip()+
  theme(axis.text = element_text(colour='black'),
        axis.text.x = element_text(angle=90,hjust = 1,vjust=0.5),
        #strip.text = element_text(angle=90,hjust = 1,vjust=0.5),
        panel.spacing = unit(0, "lines"), 
        strip.background = element_rect(),
        strip.text = element_blank())

@

\FloatBarrier
\newpage
\appendix
\renewcommand\thefigure{\thesection.\arabic{figure}}   
\setcounter{figure}{0} 
\section{A: Model fit}


\begin{figure}
\centering

<<fig.width=4,fig.height=4,out.width='0.7\\textwidth'>>=

#assessment time
devtime <- apply(cbind(year.table$Year.of.fishery.development..stock.based.,year.table$minyear),1,min,na.rm=T)

a.time <- as.numeric(year.table$Year.of.first.stock.assessment) - devtime

# true false censoring
censored <- as.logical(is.na(a.time))

# Kaplan-Meyer non-parametric survival at t - should be linear with slope p
km.cs <- survfit(Surv(a.time,!censored) ~ 1)
summary.km.cs <- summary(km.cs)
rcu <- summary.km.cs$time 
surv.cs <- summary.km.cs$surv
plot(log(rcu),log(-log(surv.cs)),type="p",pch=16,xlab="log(t)",ylab="log(-log(S(t)))")
lm_fit <- lm(log(-log(surv.cs+1e-10))~log(rcu))$coefficients

abline(a=lm_fit[1],b=lm_fit[2],col=3,lwd=2); 
@
\caption{Appropriateness of the Weibull event-time model for the time-to-assessment dataset. If the Weibull applies, the time from fishery development to assessment should fall on a line with slope $p$ (the Weibull shape parameter) between $log(-log(\hat{S}(t)))$, where $\hat{S}(t)$ is the non-parametric Kaplan-Meyer estimate of survival at time $t$, and the log of $t$. Here, $p$ evaluates to \Sexpr{round(lm_fit[2],2)}, suggesting an increasing assessment rate with increasing time $t$.}
\label{fig:Weibull_check}
\end{figure}

\begin{figure}
\centering
<<fig.width=4,fig.height=4,out.width='0.7\\textwidth'>>=

# Kaplan-Meyer non-parametric survival at CS - should follow exp(1) distribution
CS.full <- tbl_df(get_coef_chains(model.out = a.out, coef.names = 'CS'))

# just look at mean CS for now, can put posterior around it later
CS.means <- CS.full %>%
  group_by(Parameter) %>%
  summarise(post.mean = mean(MCMC))

CS = CS.means$post.mean

devtime <- apply(cbind(year.table$Year.of.fishery.development..stock.based.,year.table$minyear),1,min,na.rm=T)

a.time <- as.numeric(year.table$Year.of.first.stock.assessment) - devtime
censored <- as.logical(is.na(a.time))

km.cs <- survfit(Surv(CS,!censored) ~ 1)
summary.km.cs <- summary(km.cs)
rcu <- summary.km.cs$time # Cox-Snell residuals of
                            # uncensored points.
surv.cs <- summary.km.cs$surv
plot(rcu,-log(surv.cs),type="p",pch=16,
xlab="Cox-Snell residual",ylab="Cumulative hazard")
abline(a=0,b=1,col=3,lwd=2); 

@
\caption{Model fit of the Weibull survival model, based on Cox-Snell residuals calculated at the posterior mean of the linear predictor. For a perfect fit all data points (solid points) would lie on the y=x line.}
\end{figure}



<<surv_plot_region,fig.cap='Marginal probability of a stock being assessed as a function of time, for stocks of various taxonomic orders, class, regions and habitats. For taxonomic variables, only the eight levels with the most stocks represented in our dataset are shown. Marginal probabilities were evaluated at the mean of continuous covariates.',echo=FALSE,results='hide',fig.width=9,fig.height=5,out.width='1\\textwidth',fig.align='center'>>=

preds <- get_coef_chains(model.out = a.out, coef.names = '(mu\\[[0-9]*)')

mus <- tbl_df(preds) %>% filter(!grepl('\\.',Parameter))

mus$Parameter <- as.character(mus$Parameter)
mus$ymin <- 2016-rep(year.table$minyear,each=6000)
mus$cc <- rep(censored,each=6000)

mus.p <- mus %>% 
  split(.$Parameter) %>%
  purrr::map(function(l) {
    lmin <- unique(l$ymin)
    tp <- sapply(seq(lmin,lmin+34),function(t) 1-exp(-l$MCMC*t^tau$MCMC))
    data.frame(time=2016:2050,mm=apply(tp,2,median),censored=unique(l$cc))
  })


mpreda <- reshape2::melt(mus.p,id.vars='time') %>% dplyr::select(-variable)
names(mpreda) <- c('time','MCMC','Parameter')

preda <- tbl_df(mpreda) %>% 
  mutate(effect = do.call('rbind',strsplit(as.character(Parameter),'\\.'))[,1],
         Effect = .simpleCap(effect),
         idx = as.numeric(regmatches(Parameter,regexpr('([0-9]+)',Parameter))))


preda$Effect[preda$Effect=='Hab'] <- 'Habitat'

pred.plot <- data.frame(preda) %>%
  rowwise() %>%
  mutate(Group = levels(as.factor(year.table[,unique(Effect)]))[idx])
  
p=0;gg<- list()
for(group in unique(pred.plot$Effect)){
  
  this <- names(sort(table(year.table[,group]),decreasing = T)[1:8])
  p=p+1;
  pp<- pred.plot %>% filter(Effect == group, Group %in% this)
  
  gg[[p]] <- ggplot(pp) + 
    geom_line(aes(x=time,y=MCMC,col=Group,linetype=Group)) + 
    theme_classic() + 
    theme(axis.text = element_text(colour='black'))+
    ylab('Probability of being assessed') +
    xlab('Time (yr)') + 
    viridis::scale_color_viridis(group,discrete = T) + 
    scale_linetype_discrete(group)+
    coord_cartesian(expand = F)
}

gridExtra::grid.arrange(grobs=gg,ncol=2)

@


\end{document}